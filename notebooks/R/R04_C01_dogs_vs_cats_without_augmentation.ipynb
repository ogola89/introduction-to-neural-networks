{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R04_C01_dogs_vs_cats_without_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBFXQGKYUc4X"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors. 2020 Sheffield RSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1z4xy2gTUc4a"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Notice\n",
        "Remember to enable GPU to make everything run faster (Runtime -> Change runtime type -> Hardware accelerator -> GPU).\n",
        "Also, if you run into trouble, simply reset the entire environment and start from the beginning:\n",
        "*   Edit -> Clear all outputs\n",
        "*   Runtime -> Reset all runtimes"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE7KNzPPVrVV"
      },
      "source": [
        "# Lab 04a: Dogs vs Cats Image Classification Without Image Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwQtSOz0VrVX"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/R04_C01_dogs_vs_cats_without_augmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/R04_C01_dogs_vs_cats_without_augmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN7G9GFmVrVY"
      },
      "source": [
        "In this tutorial, we will discuss how to classify images into pictures of cats or pictures of dogs. We'll build an image classifier using a `keras_model_sequential()` model and load data using `keras` function `image_data_generator()`.\n",
        "\n",
        "## Specific concepts that will be covered:\n",
        "In the process, we will build practical experience and develop intuition around the following concepts\n",
        "\n",
        "* Building _data input pipelines_ using the `image_data_generator()` class â€” How can we efficiently work with data on disk to interface with our model?\n",
        "* _Overfitting_ - what is it, how to identify it?\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "**Before you begin**\n",
        "\n",
        "Before running the code in this notebook, reset the runtime by going to **Runtime -> Reset all runtimes** in the menu above. If you have been working through several notebooks, this will help you avoid reaching Colab's memory limits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHx57rQES5y0"
      },
      "source": [
        "# Install and load dependencies\n",
        "\n",
        "First, you'll need to install and load R package Keras which will also install TensorFlow. We'll also install package `fs` which has useful functionality for working with our filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvHtCuB1TNHd"
      },
      "source": [
        "install.packages(c(\"keras\", \"fs\"))\n",
        "library(keras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZZI6lNkVrVm"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPHx8-t-VrVo"
      },
      "source": [
        "To build our image classifier, we begin by downloading the dataset. The dataset we are using is a filtered version of <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" target=\"_blank\">Dogs vs. Cats</a> dataset from Kaggle (ultimately, this dataset is provided by Microsoft Research).\n",
        "\n",
        "In previous Colabs, we've used <a href=\"https://www.tensorflow.org/datasets\" target=\"_blank\">TensorFlow Datasets</a>, which is a very easy and convenient way to use datasets. In this Colab however, we will make use of the class `image_data_generator()` which will read data from disk. We therefore need to directly download *Dogs vs. Cats* from a URL and unzip it to the Colab filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP10Mi8KBnZT"
      },
      "source": [
        "URL <- \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
        "zip_dir <- get_file('cats_and_dogs_filtered.zip', origin = URL, extract = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqy73uxUDpTl"
      },
      "source": [
        "The dataset we have downloaded has the following directory structure.\n",
        "\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
        "<b>cats_and_dogs_filtered</b>\n",
        "|__ <b>train</b>\n",
        "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
        "|__ <b>validation</b>\n",
        "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
        "</pre>\n",
        "\n",
        "We can list the directories with the following terminal command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm1mzco4Ecfa"
      },
      "source": [
        "zip_dir_base <- dirname(zip_dir)\n",
        "fs::dir_tree(zip_dir_base, recurse = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4hy-Et8FiLX"
      },
      "source": [
        "We'll now assign variables with the proper file path for the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJosGnjvFjqJ"
      },
      "source": [
        "base_dir <- fs::path(zip_dir_base, \"cats_and_dogs_filtered\")\n",
        "train_dir <- fs::path(base_dir, \"train\")\n",
        "validation_dir <- fs::path(base_dir, \"validation\")\n",
        "\n",
        "train_cats_dir <- fs::path(train_dir, \"cats\")\n",
        "train_dogs_dir <- fs::path(train_dir, \"dogs\")\n",
        "validation_cats_dir <- fs::path(validation_dir, \"cats\")\n",
        "validation_dogs_dir <- fs::path(validation_dir, \"dogs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdrHHTy2VrV3"
      },
      "source": [
        "### Understanding our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LblUYjl-VrV3"
      },
      "source": [
        "Let's look at how many cats and dogs images we have in our training and validation directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GSeM85GC6Ar"
      },
      "source": [
        "num_cats_tr <- length(fs::dir_ls(train_cats_dir))\n",
        "num_dogs_tr <- length(fs::dir_ls(train_dogs_dir))\n",
        "\n",
        "num_cats_val <- length(fs::dir_ls(validation_cats_dir))\n",
        "num_dogs_val <- length(fs::dir_ls(validation_dogs_dir))\n",
        "\n",
        "total_train <- num_cats_tr + num_dogs_tr\n",
        "total_val <- num_cats_val + num_dogs_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njcNKT-_Djw9"
      },
      "source": [
        "cat('total training cat images:', num_cats_tr, \"\\n\")\n",
        "cat('total training dog images:', num_dogs_tr, \"\\n\")\n",
        "\n",
        "cat('total validation cat images:', num_cats_val, \"\\n\")\n",
        "cat('total validation dog images:', num_dogs_val, \"\\n\")\n",
        "cat(\"--\", \"\\n\")\n",
        "cat(\"Total training images:\", total_train, \"\\n\")\n",
        "cat(\"Total validation images:\", total_val, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdsI_L-NVrV_"
      },
      "source": [
        "# Setting Model Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lp-0ejxOtP1"
      },
      "source": [
        "For convenience, we'll set up variables that will be used later while pre-processing our dataset and training our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NqNselLVrWA"
      },
      "source": [
        "batch_size = 100  # Number of training examples to process before updating our models variables\n",
        "img_shape  = 150  # Our training data consists of images with width of 150 pixels and height of 150 pixels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INn-cOn1VrWC"
      },
      "source": [
        "# Data Preparation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jfk6aSAVrWD"
      },
      "source": [
        "Images must be formatted into appropriately pre-processed floating point tensors before being fed into the network. The steps involved in preparing these images are:\n",
        "\n",
        "1. Read images from the disk\n",
        "2. Decode contents of these images and convert it into proper grid format as per their RGB content\n",
        "3. Convert them into floating point tensors\n",
        "4. Rescale the tensors from values between 0 and 255 to values between 0 and 1, as neural networks prefer to deal with small input values.\n",
        "\n",
        "Fortunately, all these tasks can be done using the class [**`image_data_generator()`**](https://keras.rstudio.com/reference/image_data_generator.html).\n",
        "\n",
        "We can set this up in a couple of lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8nmlwgdEy4m"
      },
      "source": [
        "train_image_generator <- image_data_generator(rescale = 1/255)\n",
        "validation_image_generator <- image_data_generator(rescale = 1/255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5r8y3GS7HGk"
      },
      "source": [
        "After defining our image generators, which dictate the processing of our training and validation images, [**flow_from_directory**](https://keras.rstudio.com/reference/flow_images_from_directory.html) method creates a **data generator** which will load images from the disk, apply rescaling, and resize them using single line of code.\n",
        "\n",
        "At this stage we define how we want our images to flow from the directory (e.g. through bacth size), image properties and the class mode of our data (which is importtant information that helps the data generator create appropriate labels for our images according to our folder's file structure).\n",
        "\n",
        "As we only have two classes of images, we need to set `class_mode` to `\"binary\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvgR36FtFQUA"
      },
      "source": [
        "train_data_gen <- flow_images_from_directory(directory = train_dir,\n",
        "                                             generator = train_image_generator,\n",
        "                                             target_size = c(img_shape, img_shape),\n",
        "                                             class_mode = \"binary\",\n",
        "                                             batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI29pSjGGI2D"
      },
      "source": [
        "val_data_gen <- flow_images_from_directory(directory = validation_dir,\n",
        "                                             class_mode = \"binary\",\n",
        "                                             generator = validation_image_generator,\n",
        "                                             target_size = c(img_shape, img_shape),\n",
        "                                             batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU1CgL6s7RL_"
      },
      "source": [
        "## Exploring the data through the Image generator"
      ]
    },
    {
      "source": [
        "We can get information about our data generators by calling properties associated with them, such as `class_mode`, `labels` or `class_indices`"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uClhH72rcqb8"
      },
      "source": [
        "val_data_gen$class_mode\n",
        "val_data_gen$labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uroOYQs4Ykfk"
      },
      "source": [
        "train_data_gen$class_indices\n",
        "val_data_gen$class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "We can get the next batch of data from a data generator using `generator_next()`, in this case the first bacth will be returned."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe0eZZiFDM-n"
      },
      "source": [
        "sample_training_images <- generator_next(train_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5dVBtnIf2si"
      },
      "source": [
        "str(sample_training_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "We can also get data by indexing a data generator (note it is zero indexed, so to get the first batch you index with 0, because the underlying code it is running is python)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_training_images <- train_data_gen[0]\n",
        "str(sample_training_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhY0UhAw7xwS"
      },
      "source": [
        "We can see the structure of our batched, generated data is a list with two elements. \n",
        "- The first element contains our **features (images)** in a 4 dimensional array (batch number, height, width, channels). As we are dealing with RGB images, this time each image has 3 channels  \n",
        "- The second element contains our **targets (labels)** and is a 1d vector. This is because we set the `class_mode` to `\"binary\"`. If we'd left it as the dafault (`\"categorical\"`), it would have generated [**one-hot encoded**](https://www.youtube.com/watch?v=BecEHOVmx9o) targets (a matrix of dimensions *batch number* x *classes*) with the correct target class encoded with 1 and all other classes with 0. It's important to check for the shape of the targets as it affects the choice and effectiveness of the loss function when we compile the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-P0KZJ33L2q"
      },
      "source": [
        "plot_rgb_image <- function(image_array){\n",
        "  image_array %>%\n",
        "  array_reshape(dim = c(dim(.)[1:3])) %>%\n",
        "  as.raster(max = 1) %>%\n",
        "  plot()\n",
        "}\n",
        "\n",
        "sample_training_images[[1]][1,,,] %>%\n",
        "  plot_rgb_image()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIP0fdWj7hK6"
      },
      "source": [
        "# set plotting options\n",
        "options(repr.plot.width = 16, repr.plot.height = 4)\n",
        "\n",
        "# set number of images to plot\n",
        "n <- 4\n",
        "\n",
        "# Loop plotting over n images\n",
        "layout(matrix(1:n, ncol = 4), respect = FALSE)\n",
        "for(i in 1:n){\n",
        "  sample_training_images[[1]][i,,,] %>%\n",
        "  plot_rgb_image()\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Ej-HLGVrWZ"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEgW4i18VrWZ"
      },
      "source": [
        "## Exercise 4.1  Define the model (20 mins for 4.1, 4.2 & 4.3)\n",
        "\n",
        "The model consists of four convolution blocks with a max pool layer in each of them. Then we have a fully connected layer with 512 units, with a `relu` activation function. The model will output class probabilities for two classes â€” dogs and cats â€” using `softmax`. \n",
        "\n",
        "The list of model layers:\n",
        "* 2D Convolution - `img_shape` x `img_shape` x 3 input shape, 32 filters, 3x3 kernel, ReLU activation\n",
        "* 2D Max pooling - 2x2 kernel\n",
        "* 2D Convolution - 64 filters, 3x3 kernel, ReLU activation\n",
        "* 2D Max pooling - 2x2 kernel\n",
        "* 2D Convolution - 128 filters, 3x3 kernel, ReLU activation\n",
        "* 2D Max pooling - 2x2 kernel\n",
        "* 2D Convolution - 128 filters, 3x3 kernel, ReLU activation\n",
        "* 2D Max pooling - 2x2 kernel\n",
        "* Flatten\n",
        "* Dense - 512 nodes, ReLU activation\n",
        "* Dense - 2 nodes, softmax activation\n",
        "\n",
        "Check the documentation on [Guide to Keras Basics](https://keras.rstudio.com/articles/guide_keras.html) and [Keras functions](https://keras.rstudio.com/reference/index.html) for information on how to specify the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILW_KhJWCsS8"
      },
      "source": [
        "# TODO - Create the CNN model as specified above\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XFhAel6Wnb"
      },
      "source": [
        "### Exercise 4.1 Solution\n",
        "\n",
        "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/solutions/E.4.1.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YmQZ3TAVrWg"
      },
      "source": [
        "### Model Summary\n",
        "\n",
        "Let's look at all the layers of our network. We can view a summary of our network by simply printing the model to the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0My-ij7ao4au"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI5cdkMQVrWc"
      },
      "source": [
        "### Exercise 4.2 Compile the model\n",
        "\n",
        "As usual, we will use the `adam` optimizer. Since we output a softmax categorization, we'll use `sparse_categorical_crossentropy` as the loss function. We would also like to look at training and validation accuracy on each epoch as we train our network, so we are passing in the metrics argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsEgGQDY57bZ"
      },
      "source": [
        "# TODO - Compile the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEqNbXrI57bZ"
      },
      "source": [
        "#### Exercise 4.2 Solution\n",
        "\n",
        "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/solutions/E.4.2.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N06iqE8VVrWj"
      },
      "source": [
        "### Exercise 4.3 Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oub9RtoFVrWk"
      },
      "source": [
        "It's time we train our network. Have a look at the [documentation](https://keras.rstudio.com/reference/fit.html) to refresh how to use the `fit` function.\n",
        "\n",
        "* Since our data are coming from a training data generator, we provide that as our `x` argument.\n",
        "* Since we have a validation dataset, we can use this to evaluate our model as it trains by supplying the validation data generator  to argument `validation_data`. \n",
        "\n",
        "Fit your model for 40 epochs. Remember to assign it to a `history` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38P9hwKxCsS-"
      },
      "source": [
        "# TODO - Fit the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeWanmrjCsS-"
      },
      "source": [
        "#### Exercise 4.3 Solution\n",
        "\n",
        "The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/R/solutions/E.4.3.ipynb)"
      ]
    },
    {
      "source": [
        "Let's have a look at our history"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojJNteAGVrWo"
      },
      "source": [
        "### Visualizing results of the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZPYT-EmVrWo"
      },
      "source": [
        "We'll now visualize the results we get after training our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgH2fpZoVfCE"
      },
      "source": [
        "plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU93-ddWGW7X"
      },
      "source": [
        "As we can see from the plots, training accuracy and validation accuracy are off by large margin and our model has achieved only around 75% accuracy on the validation set (depending on the number of epochs you trained for).\n",
        "\n",
        "This is a clear indication of overfitting. Once the training and validation curves start to diverge, our model has started to memorize the training data and is unable to perform well on the validation data."
      ]
    }
  ]
}