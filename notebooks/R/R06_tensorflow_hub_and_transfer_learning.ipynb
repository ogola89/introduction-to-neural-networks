{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "R05_tensorflow_hub_and_transfer_learning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "ir",
   "display_name": "R"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_tvPdyfA-BL"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "0O_LFhwSBCjm"
   },
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Notice\n",
    "Remember to enable GPU to make everything run faster (Runtime -> Change runtime type -> Hardware accelerator -> GPU).\n",
    "Also, if you run into trouble, simply reset the entire environment and start from the beginning:\n",
    "*   Edit -> Clear all outputs\n",
    "*   Runtime -> Reset all runtimes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-3Pry4jh1-E"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/L05_tensorflow_hub_and_transfer_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/L05_tensorflow_hub_and_transfer_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxjpzKTvg_dd"
   },
   "source": [
    "# Lab 06: TensorFlow Hub and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crU-iluJIEzw"
   },
   "source": [
    "[TensorFlow Hub](http://tensorflow.org/hub) is an online repository of already trained TensorFlow models that you can use. Many of these are available through the [Keras R package](https://keras.rstudio.com/index.html)\n",
    "These models can either be used as is, or they can be used for Transfer Learning.\n",
    "\n",
    "Transfer learning is a process where you take an existing trained model, and extend it to do additional work. This involves leaving the bulk of the model unchanged, while adding and retraining the final layers, in order to get a different set of possible outputs.\n",
    "\n",
    "In this Colab we will do both.\n",
    "\n",
    "Here, you can see all the models available on [TensorFlow Hub](https://tfhub.dev/). You can find a list of models available through the [Keras R package](https://keras.rstudio.com/index.html) under [**Applications** in the reference manual](https://keras.rstudio.com/reference/index.html#section-applications)\n",
    "\n",
    "## Concepts that will be covered in this Colab\n",
    "\n",
    "1. Use a TensorFlow Hub model for prediction.\n",
    "2. Use a TensorFlow Hub model for Dogs vs. Cats dataset.\n",
    "3. Do simple transfer learning with TensorFlow Hub.\n",
    "4. Fine-tune your transferred model\n",
    "\n",
    "Before starting this Colab, you should reset the Colab environment by selecting `Runtime -> Reset all runtimes...` from menu above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RVsYZLEpEWs"
   },
   "source": [
    "# Install and load dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUCEcRdhnyWn"
   },
   "source": [
    "First, you'll need to install and load R package Keras which will also install TensorFlow. We'll also install package fs which has useful functionality for working with our filesystem."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z0rPA7CkF5l8"
   },
   "source": [
    "install.packages(c(\"keras\", \"fs\"))\n",
    "library(keras)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4YuF5HvpM1W"
   },
   "source": [
    "# Part 1: Use a TensorFlow Hub MobileNet for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Sh2sPc10V0b"
   },
   "source": [
    "In this part of the Colab, we'll take a trained model, load it and try it out.\n",
    "\n",
    "The model that we'll use is MobileNet v2 (but any model from [tf2 compatible image classifier URL from tfhub.dev](https://tfhub.dev/s?q=tf2&module-type=image-classification) would work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pUaDD4NGvSE"
   },
   "source": [
    "## Load the classifier\n",
    "\n",
    "We can load the MobileNet model and create a Keras model from it using `keras` function `application_mobilenet_v2()`.\n",
    "MobileNet is expecting images of 224 $\\times$ 224 pixels, in 3 color channels (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "66u2UZJMGlIi"
   },
   "source": [
    "model <- application_mobilenet_v2()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8KIqS0onHlSI"
   },
   "source": [
    "model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwZXaoV0uXp2"
   },
   "source": [
    "## Run it on a single image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQItP1i55-di"
   },
   "source": [
    "MobileNet has been trained on the ImageNet dataset. ImageNet has 1000 different output classes, and one of them is military uniforms.\n",
    "Let's get an image containing a military uniform that is not part of ImageNet, and see if our model can predict that it is a military uniform.\n",
    "\n",
    "First let's download and preprocess our image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cAeWHrNHH-9U"
   },
   "source": [
    "image_res = 224\n",
    "grace_hopper <- get_file('image.jpg',\n",
    "                         'https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg') %>%\n",
    "                  image_load() %>% # Load downloaded image\n",
    "                  image_array_resize(image_res, image_res)/255  # resize to fit model input_shape and rescale"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Czp8STYIPw54"
   },
   "source": [
    "Let's inspect some of the processed image properties."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_8t8bWNuJokn"
   },
   "source": [
    "range(grace_hopper)\n",
    "dim(grace_hopper)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCg_nyqJPXf2"
   },
   "source": [
    "Let's create a function to plot the image and have a look at it"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LV8Kg_69PN4T"
   },
   "source": [
    "plot_rgb_image <- function(image_array){\n",
    "  image_array %>%\n",
    "  array_reshape(dim = c(dim(.)[1:3])) %>%\n",
    "  as.raster(max = 1) %>%\n",
    "  plot()\n",
    "}\n",
    "\n",
    "plot_rgb_image(grace_hopper)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMzYMH2EJwzt"
   },
   "source": [
    "Now, let's feed the image into the mobilenet model we loaded to get a prediction. Remember, models always want a batch of images to process. So here, we add a batch dimension using `array_reshape()`, and pass the image to the model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5wheO1xsKUVW"
   },
   "source": [
    "result <- model %>%\n",
    "  predict(array_reshape(grace_hopper, c(1, dim(grace_hopper))))\n",
    "\n",
    "str(result)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MiiDseyKTCu"
   },
   "source": [
    "The result is a 1 $\\times$ 1000 matrix of logits, each column rating the probability of each class for the image.\n",
    "\n",
    "Because there is only one row, we can find the top class ID directly with `which.max()` (rather than applying `which.max` to each row). But how can we know what class this actually is and in particular if that class ID in the ImageNet dataset denotes a military uniform or something else?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "muB0MVI8NLUJ"
   },
   "source": [
    "predicted_class <- which.max(result)\n",
    "predicted_class"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9I_t9L1nNr0_"
   },
   "source": [
    "## Decode the predictions\n",
    "\n",
    "To see what our predicted_class is in the ImageNet dataset is, we can use `keras` function `imagenet_decode_predictions()`. By default it returns information on the top 5 predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Glv0jFQ4NrTm"
   },
   "source": [
    "imagenet_decode_predictions(result)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvb1fbe_OYUA"
   },
   "source": [
    "Bingo. Our model correctly predicted **military uniform** as the most probable class with a much higher probability than the next class (_suit_)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amfzqn1Oo7Om"
   },
   "source": [
    "# Part 2: Use a TensorFlow Hub models for the Cats vs. Dogs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-nIpVJ94xrw"
   },
   "source": [
    "Now we'll use the full MobileNet model and see how it can perform on the Dogs vs. Cats dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z93vvAdGxDMD"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We download the dataset again. The dataset we are using is a filtered version of <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" target=\"_blank\">Dogs vs. Cats</a> dataset from Kaggle (ultimately, this dataset is provided by Microsoft Research)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NFpoMDvrTTLN"
   },
   "source": [
    "URL <- \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "zip_dir <- get_file('cats_and_dogs_filterted.zip', origin = URL, extract = TRUE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKNVlt1MTbEf"
   },
   "source": [
    "The dataset we have downloaded has the following directory structure.\n",
    "\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>cats_and_dogs_filtered</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...]\n",
    "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
    "|__ <b>validation</b>\n",
    "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...]\n",
    "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
    "</pre>\n",
    "\n",
    "We can list the directories with the following terminal command:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZR3B2TqRV_Pr"
   },
   "source": [
    "zip_dir_base <- dirname(zip_dir)\n",
    "fs::dir_tree(zip_dir_base, recurse = 2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KnMK35BWGqC"
   },
   "source": [
    "We'll now assign variables with the proper file path for the training and validation sets. We'll also create some variables that hold information about the size of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KlP-iXwoWHRO"
   },
   "source": [
    "base_dir <- fs::path(zip_dir_base, \"cats_and_dogs_filtered\")\n",
    "train_dir <- fs::path(base_dir, \"train\")\n",
    "validation_dir <- fs::path(base_dir, \"validation\")\n",
    "\n",
    "train_cats_dir <- fs::path(train_dir, \"cats\")\n",
    "train_dogs_dir <- fs::path(train_dir, \"dogs\")\n",
    "validation_cats_dir <- fs::path(validation_dir, \"cats\")\n",
    "validation_dogs_dir <- fs::path(validation_dir, \"dogs\")\n",
    "\n",
    "\n",
    "num_cats_tr <- length(fs::dir_ls(train_cats_dir))\n",
    "num_dogs_tr <- length(fs::dir_ls(train_dogs_dir))\n",
    "\n",
    "num_cats_val <- length(fs::dir_ls(validation_cats_dir))\n",
    "num_dogs_val <- length(fs::dir_ls(validation_dogs_dir))\n",
    "\n",
    "total_train <- num_cats_tr + num_dogs_tr\n",
    "total_val <- num_cats_val + num_dogs_val"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueSIWr4z8813"
   },
   "source": [
    "Lets create an image generator to read our images from their directories which rescales our image to values from 0 to 1. Let's also create a flow from our training directories which also resizes our images to the resolution expected by our mobilenet model. Let's also set the batch to 10"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sC2GHHhiXXed"
   },
   "source": [
    "pred_image_generator <- image_data_generator(rescale = 1/255)\n",
    "\n",
    "batch_size <- 10\n",
    "pred_data_gen <- flow_images_from_directory(directory = train_dir,\n",
    "                                             generator = pred_image_generator,\n",
    "                                             target_size = c(image_res, image_res),\n",
    "                                             class_mode = \"binary\",\n",
    "                                             batch_size = batch_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z59uQkQz8vCX"
   },
   "source": [
    "Now that we've created an image generator to read our images from their directories, lets get the first batch."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5gYxrbtLX9bc"
   },
   "source": [
    "pred_batch <- pred_data_gen[0]\n",
    "str(pred_batch)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nnhsdc1zHuc_"
   },
   "source": [
    "Now, we can use model to predict a class for the images contained in the first element of the generator output. Let's also decode the prediction, returning only the top predicted class for each image, and bind all rows together into a single data.frame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zvsWasL7YA_n"
   },
   "source": [
    "predictions <- model %>%\n",
    "  predict(pred_batch[[1]]) %>%\n",
    "  imagenet_decode_predictions(top = 1) %>%\n",
    "  do.call(rbind, .)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXMsjPfNIHXY"
   },
   "source": [
    "Let's add the actual labels for each row to the data.frame and have a look at our predictions. We can see that the mobilenet predictions are alot more fine-grained but overall it seems to be doing a pretty good job at discriminating between cats and dogs!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CxmdWXUkIMnZ"
   },
   "source": [
    "predictions$label <- names(pred_data_gen$class_indices)[pred_batch[[2]] + 1]\n",
    "predictions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNgGWQLKIowr"
   },
   "source": [
    "Let's now plot the images from our Dogs vs Cats dataset and put the ImageNet predicted labels above them and their actual labels below by modifying our rgb plotting function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KMSo5SUpD0wE"
   },
   "source": [
    "plot_rgb_image_ttl <- function(image_array, prediction){\n",
    "  image_array %>%\n",
    "  array_reshape(dim = c(dim(.)[1:3])) %>%\n",
    "  as.raster(max = 1) %>%\n",
    "  plot()\n",
    "  title(main = paste0(prediction$class_description, \n",
    "                      \" (\", format(prediction$score * 100, digits = 2), \"%)\"),\n",
    "        sub = prediction$label)\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4edZOjXHG3VL"
   },
   "source": [
    "options(repr.plot.width = 16, repr.plot.height = 8)\n",
    "\n",
    "# Set the layout\n",
    "layout(matrix(1:batch_size, ncol = 5))\n",
    "\n",
    "# Loop plotting over the batch of images\n",
    "for(i in 1:batch_size){\n",
    "  plot_rgb_image_ttl(pred_batch[[1]][i,,,], predictions[i,])\n",
    "  }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzV457OXreQP"
   },
   "source": [
    "# Part 3: Do simple transfer learning with pretrained MobileNet V2 model architecture\n",
    "\n",
    "Let's now use the [MobileNet V2 model architecture](https://keras.rstudio.com/reference/application_mobilenet_v2.html) to do Transfer Learning.\n",
    "\n",
    "With transfer learning we reuse parts of an already trained model and change the final layer, or several layers, of the model, and then retrain those layers on our own dataset.\n",
    "\n",
    "In addition to the complete models, models available through `keras` listed under [applications](https://keras.rstudio.com/reference/index.html#section-applications) in the documentation can be loaded without the last classification layer. These can be used to easily do transfer learning. To do so, we set `include_top = FALSE` when loading. This specifies whether to include the fully-connected layer at the top of the network. We also set `pooling = \"avg\"`. This is an optional pooling mode for feature extraction when `include_top` is `FALSE`. `avg` means that global average pooling will be applied to the output of the last convolutional layer, and thus the output of the model will be a 2D tensor.\n",
    "\n",
    "We'll also continue to use the Dogs vs Cats dataset, so we will be able to compare the performance of this model against the ones we created from scratch earlier.\n",
    "\n",
    "Note that we're calling the partial model (without the final classification layer) a `feature_extractor`. The reasoning for this term is that it will take the input all the way to a layer containing a number of features. So it has done the bulk of the work in identifying the content of an image, except for creating the final probability distribution. That is, it has extracted the features of the image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5wB030nezBwI"
   },
   "source": [
    "feature_extractor <- application_mobilenet_v2(input_shape = c(image_res, image_res, 3),\n",
    "                                      include_top = FALSE, pooling = \"avg\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkSvAPvKOWg2"
   },
   "source": [
    "Let's run the batch of cat & dog images we've already loaded through this, and see the final shape. 10 is the number of images, and 1280 is the number of neurons in the last layer of the partial model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Of7i-35F09ls"
   },
   "source": [
    "feature_batch <- feature_extractor(pred_batch[[1]])\n",
    "dim(feature_batch)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spoop9WRUf1D"
   },
   "source": [
    "### Prepare image data generators\n",
    "\n",
    "Before we proceed let's set up our image generators. We'll use the data augmentation generators we developed in the previous lab. We'll also set our batch size to 32."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tE26AmbAU98D"
   },
   "source": [
    "batch_size <- 32\n",
    "# training generators\n",
    "train_image_generator <- image_data_generator(rescale = 1/255,\n",
    "                                              rotation_range = 45,\n",
    "                                              width_shift_range = 0.2,\n",
    "                                              height_shift_range = 0.2,\n",
    "                                              shear_range = 0.2,\n",
    "                                              zoom_range = 0.2,\n",
    "                                              horizontal_flip = TRUE,\n",
    "                                              fill_mode = 'nearest')\n",
    "\n",
    "train_data_gen <- flow_images_from_directory(directory = train_dir,\n",
    "                                             generator = train_image_generator,\n",
    "                                             target_size = c(image_res, image_res),\n",
    "                                             class_mode = \"binary\",\n",
    "                                             batch_size = batch_size)\n",
    "\n",
    "# validation generators\n",
    "val_image_generator <- image_data_generator(rescale = 1/255)\n",
    "\n",
    "val_data_gen <- flow_images_from_directory(directory = validation_dir,\n",
    "                                             generator = val_image_generator,\n",
    "                                             target_size = c(image_res, image_res),\n",
    "                                             class_mode = \"binary\",\n",
    "                                             batch_size = batch_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv47hkL7Szdd"
   },
   "source": [
    "Now, lets go back to our model and freeze the variables in the feature extractor layer, so that the training only modifies the final classifier layer. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VooJGzj5S9uM"
   },
   "source": [
    "freeze_weights(feature_extractor)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyXaEEfnTXxy"
   },
   "source": [
    "Now let's use are feature extractor as part of a keras sequential model, and add a new classification layer of 2 units with softmax activation. Note that when we print out the structure of our feature extractor we can see that the trainable parameters are 2562 and are associated with the last dense layer we have added."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KGdJNbwtTYJF"
   },
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "          feature_extractor() %>%\n",
    "          layer_dense(units = 2, activation = \"softmax\")\n",
    "model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhT9p4viV_tX"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "We now train this model like any other using an image generator, by first calling `compile` followed by `fit`. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MfZYgHQrT6vD"
   },
   "source": [
    "model %>% compile(optimizer=\"adam\",\n",
    "                  loss = 'sparse_categorical_crossentropy',\n",
    "                  metrics = \"accuracy\")\n",
    "\n",
    "epochs = 6\n",
    "history <- model %>%\n",
    "              fit(x = train_data_gen,\n",
    "              epochs = epochs,\n",
    "              validation_data = val_data_gen)\n",
    "\n",
    "cat('Validation loss:', format(tail(history$metrics$val_loss, 1), digits = 2), \"\\n\")\n",
    "cat('Validation accuracy:', format(tail(history$metrics$val_accuracy, 1), digits = 2), \"\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCWHyl68WbRO"
   },
   "source": [
    "You can see we get ~98% validation accuracy, which is absolutely awesome. This is a huge improvement over the model we created in the previous lesson, where we were able to get ~77% accuracy after 40 epochs of training. The reason for this difference is that MobileNet was carefully designed over a long time by experts, then trained on a massive dataset (ImageNet).\n",
    "\n",
    "Let's plot the training and validation accuracy/loss graphs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "At2vdIHwWmWo"
   },
   "source": [
    "# Plot history\n",
    "options(repr.plot.width = 12, repr.plot.height = 6)\n",
    "plot(history)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvavl_OaW2SN"
   },
   "source": [
    "What is a bit curious here is that validation performance is better than training performance, right from the start to the end of execution.\n",
    "\n",
    "One reason for this is that validation performance is measured at the end of the epoch, but training performance is the average values across the epoch.\n",
    "\n",
    "The bigger reason though is that we're reusing a large part of MobileNet which is already trained on Dogs and Cats images. While doing training, the network is still performing image augmentation on the training images, but not on the validation dataset. This means the training images may be harder to classify compared to the normal images in the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2vtXnjwZiTR"
   },
   "source": [
    "## Check the predictions\n",
    "\n",
    "To redo the plot from before, let's use our new model to make some predictions on the images from our `pred_batch`. \n",
    "\n",
    "Let's compile our predictions into a data.frame. Let's also add the actual labels for each row to the data.frame and have a look at our predictions. We can see that predictions using our transfer learning very accurate with very high confidence in each prediciton!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sk1RtPl1ZwgQ"
   },
   "source": [
    "pred_mat <- model %>%\n",
    "  predict(pred_batch[[1]]) \n",
    "\n",
    "predictions <- data.frame(class_description = names(pred_data_gen$class_indices)[apply(pred_mat, 1, which.max)],\n",
    "                          score = apply(pred_mat, 1, max),\n",
    "                          label = names(pred_data_gen$class_indices)[pred_batch[[2]] + 1])\n",
    "predictions"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yboppifIZwgc"
   },
   "source": [
    "Let's now plot the images from our Dogs vs Cats dataset and put the ImageNet labels above them and their actual labels below by modifying our rgb plotting function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vDRA0HR0Zwgd"
   },
   "source": [
    "options(repr.plot.width = 16, repr.plot.height = 8)\n",
    "\n",
    "# Set the layout\n",
    "layout(matrix(1:10, ncol = 5))\n",
    "\n",
    "# Loop plotting over the batch of images\n",
    "for(i in 1:10){\n",
    "  plot_rgb_image_ttl(pred_batch[[1]][i,,,], predictions[i,])\n",
    "  }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvBCUlAF-WDP"
   },
   "source": [
    "## Fine-tuning the transferred model\n",
    "\n",
    "In our previous example, we froze the `feature_extractor` so that its weights does not update during training. In some cases, performance can be further improved by doing further training of the layer, this is called fine-tuning.\n",
    "\n",
    "This fine-tuning process is done after we've trained the model with our custom classifier. First we'll want to unfreeze the `feature_extractor`. We can see now that model parameters associated with the `feature_extractor` are now also trainable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HML666d--WDQ"
   },
   "source": [
    "unfreeze_weights(feature_extractor)\n",
    "model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWGU3J9j-WDQ"
   },
   "source": [
    "Then re-compile the model. As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hF_ur7LY-WDQ"
   },
   "source": [
    "model %>% compile(optimizer = optimizer_adam(lr = 0.0001),\n",
    "                  loss = 'sparse_categorical_crossentropy',\n",
    "                  metrics = \"accuracy\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ckih63qjTtc"
   },
   "source": [
    "Finally, re-fit the model, print the validation metrics and plot model fit history"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cdt0uyjdjUwr"
   },
   "source": [
    "epochs = 6\n",
    "history <- model %>%\n",
    "              fit(x = train_data_gen,\n",
    "              epochs = epochs,\n",
    "              validation_data = val_data_gen)\n",
    "\n",
    "cat('Validation loss:', format(tail(history$metrics$val_loss, 1), digits = 2), \"\\n\")\n",
    "cat('Validation accuracy:', format(tail(history$metrics$val_accuracy, 1), digits = 2), \"\\n\")\n",
    "\n",
    "# Plot history\n",
    "options(repr.plot.width = 12, repr.plot.height = 6)\n",
    "plot(history)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}